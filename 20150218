% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009



\documentclass{acm_proc_article-sp}
\usepackage{graphicx}
\begin{document}

\title{Caching Compact Snippet in Flash-based Search Engines}
\author{
}

\maketitle

\begin{abstract}
Modern search engine answers billions of queries per day. For each query, search engine returns top-$k$ relevant results, each of which contains a small piece of text, called snippet, extracted from the corresponding document. It is very expensive to obtain a snippet because it requires both document retrieval (disk access) and string matching (CPU computation). Therefore, in order to alleviate the query latency, frequently requested documents and snippets are cached.

Recent studies showed that, however, with the trend of using solid state drive to replace hard disk drive, the bottleneck of snippet generation shifts from I/O to computation. Such finding suggests that it is crucial to improve the effectiveness of snippet caching approaches. In this paper, we propose a new data structure to compactly represent the snippet; this allows seven times or more snippets to be held within the same amount of cache space whatever caching policy is used. The experimental results show that our compact snippet cooperates well with both static and dynamic cache strategies. Besides, it is worth noticing that our method does not affect the quality of snippets generated.
\end{abstract}

% A category with the (minimum) three required fields
\category{H.3.3}{Information Storage and Retrieval}{Information
Search and Retrieval-Search process}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%\terms{Theory}

\keywords{Cache, Snippet, Solid State Drive} % NOT required for Proceedings

\section{Introduction}
Caching is an important approach to reduce the query latency in search engine architectures. A large-scale search engine typically consists of three sets of servers \cite{2,5}: Web Servers, Index Servers and Document Servers. Web server, with a query result cache (QRC), interacts with users and coordinates the whole query processing. Index server matches top-\textit{k} documents that are most related to the query. Document server is responsible for generating query result pages, which include the title, URL, and a snippet for each of the  top-\textit{k} documents. For a given query, a snippet is the portion of a document which matches the query best and the query terms are highlighted in snippets. In order to alleviate the query latency, frequently requested documents and snippets are cached. This two in-memory caches are called Document Cache (DC) \cite{10} and Snippet Cache (SC) \cite{3} respectively. DC stores document pages to save disk accesses when generating  snippets, while SC stores snippets for top-\textit{k} documents to avoid repeated computation and disk access.

Recently, solid state drive (SSD) has been extensively noticed to the public as secondary storage medium. Many companies begin to replace their hard disk drive (HDD) by SSD \cite{11}. As SSD has properties such as fast read (ten times to one hundred times faster than HDD) and write \cite{4}, it can benefit the search engine for shorter query latency and higher throughput. Snippet generation \cite{10} is an expensive process in terms of both disk access and CPU computation. In traditional HDD-based search engine architectures, the latency of snippet processing is dominated by disk access operations. As SSD has the feature of faster read (especially random read), snippet calculation time becomes the bottleneck in snippet processing \cite{11}. In this situation, how to increase the efficiency of snippet caching for the sake of avoiding duplicate computation has becoming a more important problem.

In this paper, instead of caching the full snippet, we introduce a new compact data structure to represent the snippet. During the snippet processing, we calculate the snippets in the standard way (without loss of quality), and reorganize the snippets utilizing the fact that snippet is actually a portion of document. With this new data structure, we can cache more calculation results in the limited memory space. Since the CPU time of snippet generation becomes the bottleneck of document server, this compact form of snippet will be more suitable for SSD-based search engine. The experimental results show that our method can achieve a good performance with both static and dynamic cache strategies.

The contributions of our work include:

%\vspace{-1em}
\begin{itemize}
  \item We propose a new data structure, \textit{fragment}, to represent the snippet. Fragment represents the snippet by recording its offset and length in the document. Referring to the document, caching fragment is more space-effective than storing strings directly.
  \item We introduce a new kind of query processing, \textit{fragment processing}, which uses fragment as a cached item instead of snippet (see Figure 2). During fragment processing, we can cache seven times or more snippets in the same amount of cache space and save more calculation cost. Besides, the overhead of recovering snippet from fragment and document is negligible.
\end{itemize}


\section{Related Work}
Snippet generating is one of the most crucial and time-consuming process in document servers. The problem of reducing snippet processing latency has been studied from different directions. Liu \textit{et al}. \cite{7} used a CPU-GPU hybrid system to accelerate snippet generation. Ceccarelli \textit{et al}. \cite{3} introduced an effective method named supersnippet, but this method may degrade the accuracy of snippet. Turpin \textit{et al}. \cite{10} used zlib to compress document and reduced snippet generation latency by 58\%, their method can dramatically reduce I/O cost in HDD-based search engine. Tsegay \textit{et al}. \cite{9} stored surrogates instead of the whole document to reduce I/O time.

Once SSD is used to replace HDD, methods which reduce I/O time may not help. Wang \textit{et al}. \cite{11} found that document cache, which may largely reduce I/O of document server, is not so effective in SSD-based search engine as it is in HDD-based search engine.
Thus, the snippet generation time become the new bottleneck.
In order to address this issue, we adopt a relative representation, motivated by Hoobin \textit{et al}.~\cite{6},
to allow more snippets to be cached, thus fewer need to be generated.
To the best of our knowledge, this is the first study that takes the advantage of features of SSD to improve the efficiency of snippet cache.
Similar to our study, Tong \textit{et al}. \cite{8} also took advantage of the characteristic of SSD to improve
the caching effectiveness in search engines, except that they only concerned list caching.

\section{Fragment Cache}
The using of SSD dramatically reduce the disk accessing cost of snippet processing. As a result, the CPU computation cost is playing a more important role in the snippet generation processing. Thus the designing of caches in document servers is supposed to take this change into consideration. In document servers, caching snippets, which can help refrain from repetitive computations, will be far more powerful in flash-based search engines.

\begin{figure}[h]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{figure2-yinying.pdf}\\
  \caption{Snippet structure.
  }\label{Figure 1}
\end{figure}

We introduce a new data structure called \textit{fragment} to record the information of snippets. We reorganize a snippet by considering the relative factorization of the snippet and the corresponding document to form a fragment, as shown in Figure 1. Namely, fragment is a compact form of snippet, which consists of the position information of a snippet. When calculating a fragment, we operate a standard query-biased snippet generation followed with a reorganization operation. The reorganization operation records where the snippet and highlighted terms locate in the document to form a fragment. Figure 1 shows the structure of a snippet, and the fragment of this snippet will be represented as
\[
<DocID, Frag_s, Frag_e, Highlight_s, Hignlight_e>
\]
\textit{DocID} records the document ID of the corresponding document of the fragment. $Frag_s$ records the start offset of snippet in a document, and equals to \textit{offset}. $Frag_e$ is the end offset of the snippet, and equals to \textit{offset $+$ length}. $Highlight_s$ and $Highlight_e$ are two arraylists which hold one or more starting offsets and ending offsets of highlighted terms. In the example of Figure 1, $Highlight_s$ is recorded as $[\textit{offset}_1, \textit{offset}_2]$, and $Highlight_e$ is represented as $[\textit{offset}_1+length_1, \textit{offset}_2+length_2]$. This method of generating fragment is independent with snippet generation algorithms, hence snippet generated by any methods can be represented using such fragment form. Caching fragments instead of the full snippets could reduce the size of cached items to a large extent, hence increasing the number of snippets in the cache and saving more computation cost.

\begin{figure}[h]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{figure1.pdf}\\
  \caption{Snippet processing and fragment processing.
  }\label{Figure 2}
\end{figure}

Figure 2 shows the snippet processing and the fragment processing handled by document servers. The traditional snippet processing consists of two phases: \texttt{document retrieval} and \texttt{snippet calculation}. After a pair of query and document ID is submitted to a document server, snippet cache is checked at first in the snippet processing. And a hit in snippet cache will save both the document retrieval and snippet calculation. In contrast, the fragment processing has three steps: \texttt{document retrieval}, \texttt{fragment calculation} and \texttt{snippet recovering}. In the case of fragment processing, we check fragment cache, and a miss in fragment cache will  be followed with a fragment calculating operation. Since only the position information of the snippet in a document is recorded in the fragment cache, a hit in fragment cache still needs the corresponding document to recover a full snippet. The recover operation simply extracts the substrings from a document and inserts the highlight labels by the position information recorded in the fragment. The overhead of recovering is low and can be neglected. As documents are used more frequently in the fragment processing, it may requires more document retrieval operations, which leads to more disk accesses. But the faster read speed of SSD will offset this weakness and guarantee the good performance of fragment processing. More requirements of documents also makes the document cache more important than it was in snippet processing.

In our sandbox search engine (we will describe the setting later), the average size of snippet is 375 Bytes while that of fragment is only 49 Bytes. By using fragment to represent snippet, seven times or more items can be held within the same amount of cache space, which will increase the cache hit ratio. Thus, more reduplicate CPU computation operations can be skipped.
Nevertheless, due to the succinct representation of fragment, document accessing is required each time a snippet is requested even if the corresponding fragment is found in the fragment cache. Therefore, the I/O latency will increase substantially if these corresponding documents are not cached as well. However, it is worth noticing that these documents are also popular documents themselves. In fact they may already cached in document cache (otherwise they will be fetched from the disk at the very first cache miss and inevitably will reside in the document cache). So there are almost no overhead for recovering the snippet from the fragment. Furthermore, the memory saved by using fragment cache can be reserved for caching more popular documents, which in turn improve the performance.

\section{Performance Evaluation}
We evaluate our fragment cache by comparing its snippet generation latency with that of snippet cache. We test both static and dynamic cache strategies.
\subsection{Experimental setup}
In our experiments, we use a real collection of 12 million Web pages crawled by Sogou Labs\footnote{ http://www.sogou.com/labs/resources.html?v=1}. The experiments are performed by replaying real queries from Sogou search engine. The query log contains 2M queries. The first half is used as training set to warm the cache and the second half is used as test set. We deploy a Lucene\footnote{http://lucene.apache.org} based search engine, in which our fragment cache implementation is embedded. The server is a Intel Xeon E5645(@2 .4GHz) machine, with 12GB of main memory and Windows 7 SP1 installed. We carry out experiments on a 120GB OCZ Vertex-3 SSD. To verify the effectiveness of fragment cache on different storage mediums, we also perform some experiments on a 500GB WDC HDD (5400rpm).

\subsection{Experimental results}
To evaluate the performance of fragment cache comprehensively, fragment cache is tested with both static and dynamic cache strategies. For static cache, we test QTF (Query Term Frequency) and QTFDF \cite{1}; for dynamic cache, LRU (Least
Recently Used) and LFU (Least Frequently Used) are tested. The policies mentioned above are used in both snippet (fragment) cache and document cache. In document server, memory is shared by snippet cache and document cache. We test
snippet generation latency under the conditions of different number of snippets are cached. And the fragment cache is set at the same memory space with snippet cache.

\begin{figure}[h]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{static.pdf}\\
  \caption{Performance of static cache strategies (the cache size is in terms of snippets, $\times$1000).
  }\label{Figure 3}
\end{figure}

\begin{figure}[h]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{dynamic.pdf}\\
  \caption{Performance of dynamic cache strategies (the cache size is in terms of snippets, $\times$1000).
  }\label{Figure 4}
\end{figure}

In static cache strategies, we fill the caches in document server using QTFDF and QTF according to training set. Figure 3 compares the performance of snippet cache and fragment cache. Snippet generation latency of QTFDF and QTF both declines when we allocate more memory to snippet cache. However the situation changes if fragment cache is used. Snippet generation latency decreases at first. However, when we allocate 60\% of memory to fragment cache, most of the popular fragments are cached. Allocating more memory to fragment cache does not improve its hit ratio remarkably, but results in more document cache misses which harms snippet generation latency. Therefore, when the ratio of fragment cache size to document cache size exceeds 6 : 4, the snippet generation latency goes up slowly as the ratio increases. We can conclude from Figure 2 that when the query stream contains many distinct snippets that can not be hold completely by snippet cache/fragment cache, fragment cache will show its performance advantage. Otherwise, you should choose snippet cache.

Figure 4 shows the performance of fragment cache with dynamic cache strategies. In dynamic cache strategies, items can be dynamically admitted into the cache in both training process and testing process, cache efficiency can be improved anytime. Fragment cache performs much better than snippet cache with LRU, which is 26\% faster than snippet cache. While with LFU fragment cache performs not so well as that with LRU. LFU favors the old items because it deletes the least frequently used items when cache is full, making new document item hardly be inserted into cache. Fragment cache makes the situation worse and degrades the efficiency of document cache. In all, fragment cache performs not well enough in frequency-based cache strategy.

\section{conclusion}

\bibliographystyle{abbrv}
\bibliography{fc}

\end{document}
